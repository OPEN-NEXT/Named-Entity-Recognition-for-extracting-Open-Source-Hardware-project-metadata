#! /usr/bin/env python
#  -*- coding: utf-8 -*-
#
# Support module generated by PAGE version 6.0.1
#  in conjunction with Tcl version 8.6
#    Apr 09, 2021 03:30:41 PM CEST  platform: Windows NT

import sys
from spacy import displacy
import spacy
import requests
from bs4 import BeautifulSoup
import pandas as pd
import pickle

try:
    import Tkinter as tk
except ImportError:
    import tkinter as tk

try:
    import ttk
    py3 = False
except ImportError:
    import tkinter.ttk as ttk
    py3 = True


def url_to_transcript(url):
    '''Returns transcript data specifically from scrapsfromtheloft.com.'''
    page = requests.get(url).text
    soup = BeautifulSoup(page, "lxml")
    text = [p.text for p in soup.find(class_="mw-content-ltr")]
    #print(url)
    return text

def set_Tk_var():
    global corpus
    corpus = tk.StringVar()

def init(top, gui, *args, **kwargs):
    global w, top_level, root
    w = gui
    top_level = top
    root = top

def runButton():
    url = corpus.get()
    urls = []
    urls.append(url)
    transcripts = [url_to_transcript(u) for u in urls]
    hardwares = ["hardware"]

    for i, c in enumerate(hardwares):  # creating the txt file to load the data
        with open(c + ".txt", "wb") as file:
            pickle.dump(transcripts[i], file)

    data = {}
    for i, c in enumerate(hardwares):
        with open(c + ".txt", "rb") as file:
            data[c] = pickle.load(file)

    def combine_text(list_of_text):
        '''Takes a list of text and combines them into one large chunk of text.'''
        combined_text = ' '.join(list_of_text)
        return combined_text

    data_combined = {key: [combine_text(value)] for (key, value) in data.items()}

    pd.set_option('max_colwidth', 150)
    data_df = pd.DataFrame.from_dict(data_combined).transpose()
    data_df.columns = ['transcript']
    data_df = data_df.sort_index()

    import re
    import string

    def clean_text_round1(text):
        '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''
        text = text.lower()
        text = re.sub('\[.*?\]', '', text)
        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
        return text

    round1 = lambda x: clean_text_round1(x)

    data_clean = pd.DataFrame(data_df.transcript.apply(round1))

    def clean_text_round2(text):
        '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''
        text = re.sub('[‘’“”…]', '', text)
        text = re.sub('\n', ' ', text)
        # text = re.sub('\w*\d\w*', '', text)
        return text

    round2 = lambda x: clean_text_round2(x)

    data_clean = pd.DataFrame(data_clean.transcript.apply(round2))
    # print(data_clean)

    cleaned_transcripts = [data_clean.transcript.loc[i] for i in hardwares]

    for i, c in enumerate(hardwares):
        with open(c + ".txt", "w") as file:
            file.write(cleaned_transcripts[i])

    for i, c in enumerate(hardwares):
        with open(c + ".txt", "r") as file:
            example = file.read()

    print("I got the corpus")
    nlp = spacy.load("/NERapp/best_model")
    print("NER loaded")
    doc = nlp(example)

    html = displacy.render(doc, style='ent')
    with open("data_visualisation2.html", "w") as file:
        file.write(html)
    print("html page created")
    file.close()

    import webbrowser
    new = 2
    url = "C:/Users/nicesimo/Desktop/NLP/Model/NERapp2/data_visualisation2.html"
    webbrowser.open(url,new=new)
    print("html page opened")
    sys.stdout.flush()

def destroy_window():
    # Function which closes the window.
    global top_level
    top_level.destroy()
    top_level = None

if __name__ == '__main__':
    import runModel2
    runModel2.vp_start_gui()




