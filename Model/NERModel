import json

#import the labeled data saved as a json file

labeled_data = []
with open(r"project_1_dataset_v4.jsonl", "r", encoding='utf-8') as file:
    for line in file:
        data = json.loads(line)
        labeled_data.append(data)

#change a bit the format of the json datas

TRAINING_DATA = []
for entry in labeled_data:
    entities = []
    for e in entry['labels']:
        entities.append((e[0], e[1], e[2]))
    spacy_entry = (entry['text'], {"entities": entities})
    TRAINING_DATA.append(spacy_entry)



#start the trainning process

import spacy
import random
from spacy.training import Example

nlp = spacy.blank("en")
ner = nlp.create_pipe("ner")
nlp.add_pipe('ner')
for _, annotations in TRAINING_DATA:                    #goes through all the entities are get the name token.label_ one
    for ent in annotations.get("entities"):
        ner.add_label(ent[2])
print("label added")

# Start the training
nlp.begin_training()
# Loop for 40 iterations
for itn in range(40):
    # Shuffle the training data
    random.shuffle(TRAINING_DATA)
    losses = {}
# Batch the examples and iterate over them
    for text, annotations in TRAINING_DATA:
        # create Example
        doc = nlp.make_doc(text)
        example = Example.from_dict(doc, annotations)
        # Update the model
        nlp.update([example], losses=losses, drop=0.2)
    #nlp.update(texts, annotations, losses=losses, drop=0.3)
    print(losses)

#save the model as file
nlp.to_disk("best_model")
